Instalacion de Haddoo 3.2.1 en Ubuntu 20.04

instalación JDK Versión 8
Ejecutamos los siguientes comandos:
sudo apt update (actualizamos la lista de repositorios y descargas)

Instalamos jdk 
sudo apt install openjdk-8-jdk

Validamos la versión con el siguiente comando:
java -version

Validamos si el archivo jdk esta en la ruta de enlase
ls /usr/lib/jvm

digitamos el siguiente comando para crear la variable de ambiente JAVA_HOME
export JAVA_HOME =  /usr/lib/jvm/java-8-openjdk-amd64

Validamos que ya existe la variable y tiene valor con el siguiente comando:
echo $JAVA_HOME

Instalación ssh
sudo apt install ssh
sudo apt install pdsh

Descargamos el archivo targz de la página oficial de haddoop
hadoop.apache.org/downloads

Una vez descargado nos dirigimos al directorio donde se encuentra el archivo comprimido
 y digitamos el siguiente comando para extraer la información
 
tar -zxvf hadoop-3.2.1.tar.gz

Ingresamos a la carpeta del proyecto hadoop
accedemos a la ruta
cd etc/hadoop/
gedit hadoop-env.sh

buscamos a linea comentada de la variable de entorno 
# export JAVA_HOME="/usr/lib/jvm/java-8-openjdk-amd64" (descomentamos quitando el signo #)

En la misma ruta digitamos
gedit core-site.xml

Modificamos las etiquetas configuration de la siguiente manera
<configuration>
    <property>
        <name>fs.defaultFS</name>
        <value>hdfs://localhost:9000</value>
    </property>
</configuration>

ahora abrimos el archivo hdfs-site.xml
y modificamos la etiqueta configuration:

<configuration>
    <property>
        <name>dfs.replication</name>
        <value>1</value>
    </property>
</configuration>

CONFIGURACIÓN SSH LLAVE PRIVADA Y LLAVE PÚBLICA
Command:
	ssh-keygen -t rsa -P "-f ~/.ssh/id_rsa"
	cat ~/.ssh/id_rsa.pub >> ~/.ssh/authorized_keys
        chmod 0600 ~/.ssh/authorized_keys
	
Estando dentro de la carpeta de hadoop aplicamos formato a namenode 

bin/hdfs namenode -format

generamos una nueva variable de entorno llamada PDSH_RCMD_TYPE

	export PDSH_RCMD_TYPE=ssh
	
Iniciando servidor dfs
sbin/start-dfs.sh
validamos en nuestro localhost:9870

Crear carpetas del sistema de archivos distribuidos de Hadoop
	bin/hdfs dfs -mkdir /user
	bin/hdfs dfs -mkdir /user/nameUser   (reemplazamos nameUser por el nombre de nuestro usuario hadoop)
	
Copiar archivo de entrada en el sistema de archivos distribuidos:
	bin/hdfs dfs -mkdir input
	bin/hdfs dfs -put etc/hadoop/*.xml input
	
Si aun esta corriendo nuestro dfs service lo paramos con el siguiente comando:
	sbin/stop-dfs.sh

Modificamos el archivo mapred-site.xml
	nano etc/hadoop/mapred-site.xml
	
Reemplazamos la etiqueta configuration
	<configuration>
	    <property>
		<name>mapreduce.framework.name</name>
		<value>yarn</value>
	    </property>
	    <property>
		<name>mapreduce.application.classpath</name>
		<value>$HADOOP_MAPRED_HOME/share/hadoop/mapreduce/*:$HADOOP_MAPRED_HOME/share/hadoop/mapreduce/lib/*</value>
	    </property>
	</configuration>	

nano etc/hadoop/yarn-site.xml
<configuration>
    <property>
	<name>yarn.nodemanager.aux-services</name>
	<value>mapreduce_shuffle</value>
    </property>
    <property>
	<name>yarn.nodemanager.env-whitelist</name>
	<value>JAVA_HOME,HADOOP_COMMON_HOME,HADOOP_HDFS_HOME,HADOOP_CONF_DIR,CLASSPATH_PREPEND_DISTCACHE,HADOOP_YARN_HOME,HADOOP_MAPRED_HOME</value>
    </property>
</configuration>

iniciamos el servicio yarn y dfs de hadoop
sbin/start-yarn.sh
sbin/start-dfs.sh
Validamos el arranque en el puerto localhost:8088 y localhost:9870 de nuestro navegador

De igual manera podemos iniciar todos los servicios de hadoop con el siguiente comando:
sbin/start-all.sh
	
	
	FIN!!!
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	












	
	
	
	
	


